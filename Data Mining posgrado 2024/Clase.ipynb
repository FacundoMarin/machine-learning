{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 1\n",
    "\n",
    "En este notebook, realizaremos los ejercicios planteados [ac치](https://colab.research.google.com/drive/175WnSI6Lw66rlDzBGzehTJaltnT6oGts?usp=sharing#scrollTo=XjaiegV9lvwM).\n",
    "\n",
    "## Ejercicio 1\n",
    "\n",
    "Aplicar los pasos de arriba en las siguientes oraci칩nes:\n",
    "\n",
    "Oraci칩n 1: To be or not to be.\n",
    "\n",
    "Oraci칩n 2: Common words like its, an, the, for, and that, are all considered stop words. While they're important for communicating verbally, stop words typically carry little importance to SEO and are often ignored by search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\facum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'be', 'or', 'not', 'to', 'be', '.']\n",
      "['common', 'words', 'like', 'its', ',', 'an', ',', 'the', ',', 'for', ',', 'and', 'that', ',', 'are', 'all', 'considered', 'stop', 'words', '.', 'while', 'they', \"'re\", 'important', 'for', 'communicating', 'verbally', ',', 'stop', 'words', 'typically', 'carry', 'little', 'importance', 'to', 'seo', 'and', 'are', 'often', 'ignored', 'by', 'search', 'engines', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"To be or not to be.\"\n",
    "sentence2 = \" Common words like its, an, the, for, and that, are all considered stop words. While they're important for communicating verbally, stop words typically carry little importance to SEO and are often ignored by search engines.\"\n",
    "\n",
    "sentence1_lc = [word.lower() for word in nltk.word_tokenize(sentence1)]\n",
    "sentence2_lc = [word.lower() for word in nltk.word_tokenize(sentence2)]\n",
    "\n",
    "print(sentence1_lc)\n",
    "print(sentence2_lc)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['common', 'word', 'like', 'consid', 'stop', 'word', \"'re\", 'import', 'commun', 'verbal', 'stop', 'word', 'typic', 'carri', 'littl', 'import', 'seo', 'often', 'ignor', 'search', 'engin']\n"
     ]
    }
   ],
   "source": [
    "stems1, stems2 = [], []\n",
    "for token in sentence1_lc:\n",
    "    # Process each token\n",
    "    if token not in string.punctuation:\n",
    "        if token not in stopwords:\n",
    "            token = stemmer.stem(token)\n",
    "            stems1.append(token)\n",
    "\n",
    "for token in sentence2_lc:\n",
    "    # Process each token\n",
    "    if token not in string.punctuation:\n",
    "        if token not in stopwords:\n",
    "            token = stemmer.stem(token)\n",
    "            stems2.append(token)\n",
    "\n",
    "print(stems1)\n",
    "print(stems2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['common', 'word', 'like', 'considered', 'stop', 'word', \"'re\", 'important', 'communicating', 'verbally', 'stop', 'word', 'typically', 'carry', 'little', 'importance', 'seo', 'often', 'ignored', 'search', 'engine']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas1, lemmas2 = [], []\n",
    "for token in sentence1_lc:\n",
    "    # Process each token\n",
    "    if token not in string.punctuation:\n",
    "        if token not in stopwords:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            lemmas1.append(lemma)\n",
    "\n",
    "for token in sentence2_lc:\n",
    "    # Process each token\n",
    "    if token not in string.punctuation:\n",
    "        if token not in stopwords:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            lemmas2.append(lemma)\n",
    "\n",
    "print(lemmas1)\n",
    "print(lemmas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To to To\n",
      "be be be\n",
      "or or or\n",
      "not not not\n",
      "to to to\n",
      "be be be\n",
      ". . .\n",
      "Common common Common\n",
      "words word word\n",
      "like like like\n",
      "its it it\n",
      ", , ,\n",
      "an an an\n",
      ", , ,\n",
      "the the the\n",
      ", , ,\n",
      "for for for\n",
      ", , ,\n",
      "and and and\n",
      "that that that\n",
      ", , ,\n",
      "are are are\n",
      "all all all\n",
      "considered consid considered\n",
      "stop stop stop\n",
      "words word word\n",
      ". . .\n",
      "While while While\n",
      "they they they\n",
      "'re 're 're\n",
      "important import important\n",
      "for for for\n",
      "communicating commun communicating\n",
      "verbally verbal verbally\n",
      ", , ,\n",
      "stop stop stop\n",
      "words word word\n",
      "typically typic typically\n",
      "carry carri carry\n",
      "little littl little\n",
      "importance import importance\n",
      "to to to\n",
      "SEO seo SEO\n",
      "and and and\n",
      "are are are\n",
      "often often often\n",
      "ignored ignor ignored\n",
      "by by by\n",
      "search search search\n",
      "engines engin engine\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "for token in nltk.word_tokenize(sentence1):\n",
    "    print(token + \" \" + stemmer.stem(token) + \" \" + lemmatizer.lemmatize(token))\n",
    "\n",
    "for token in nltk.word_tokenize(sentence2):\n",
    "    print(token + \" \" + stemmer.stem(token) + \" \" + lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "be\n",
      "or\n",
      "not\n",
      "to\n",
      "be\n",
      ".\n",
      " \n",
      "common\n",
      "word\n",
      "like\n",
      "its\n",
      ",\n",
      "an\n",
      ",\n",
      "the\n",
      ",\n",
      "for\n",
      ",\n",
      "and\n",
      "that\n",
      ",\n",
      "be\n",
      "all\n",
      "consider\n",
      "stop\n",
      "word\n",
      ".\n",
      "while\n",
      "they\n",
      "be\n",
      "important\n",
      "for\n",
      "communicate\n",
      "verbally\n",
      ",\n",
      "stop\n",
      "word\n",
      "typically\n",
      "carry\n",
      "little\n",
      "importance\n",
      "to\n",
      "seo\n",
      "and\n",
      "be\n",
      "often\n",
      "ignore\n",
      "by\n",
      "search\n",
      "engine\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "doc1 = nlp(sentence1)\n",
    "doc2 = nlp(sentence2)\n",
    "for token in doc1:\n",
    "    print(token.lemma_)\n",
    "for token in doc2:\n",
    "    print(token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
